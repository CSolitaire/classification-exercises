{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Exercises \n",
    "#### Corey Solitaire\n",
    "#### 9/8/2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from math import sqrt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 In a jupyter notebook, classification_exercises.ipynb, use a python module (pydata or seaborn datasets) containing datasets as a source from the iris data. Create a pandas dataframe, df_iris, from this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = sns.load_dataset('iris')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   - print the first 3 rows\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###     - print the number of rows and columns (shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###    - print the column names\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###    - print the data type of each column\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#more info is necessary\n",
    "iris.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###     - print the summary statistics for each of the numeric variables. Would you recommend rescaling the data based on these statistics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.sepal_length.var(),\n",
    "iris.sepal_width.var(),\n",
    "iris.petal_length.var(),\n",
    "iris.petal_width.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Petal length is much more variable then other measurments, I might reccomend scaling depending on the factors being examined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Read the Table1_CustDetails table from the Excel_Exercises.xlsx file into a dataframe named df_excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer = pd.read_excel('Spreadsheets_Exercises.xlsx')\n",
    "customer.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###      -assign the first 100 rows to a new dataframe, df_excel_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_excel_sample = customer.head(100)\n",
    "df_excel_sample.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###      -print the number of rows of your original dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(customer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alternate option\n",
    "customer.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###      -print the first 5 column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Select First 5 columns:\")\n",
    "print(customer[['customer_id', 'gender', 'is_senior_citizen', 'partner', 'dependents']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternate method\n",
    "\n",
    "customer.columns[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###      -print the column names that have a data type of object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredColumns = customer.dtypes[customer.dtypes == np.object]\n",
    "listOfColumnNames = list(filteredColumns.index)\n",
    "print(listOfColumnNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###     -compute the range for each of the numeric variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transcribes table\n",
    "stats =customer._get_numeric_data().describe().T\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats['range'] = stats['max'] - stats['min']\n",
    "stats[['mean', '50%', 'std', 'range']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Read the data from this google sheet into a dataframe, df_google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_url = 'https://docs.google.com/spreadsheets/d/1Uhtml8KY19LILuZsrDtlsHHDC9wuDGUSe8LTEwvdI5g/edit#gid=341089357'    \n",
    "\n",
    "csv_export_url = sheet_url.replace('/edit#gid=', '/export?format=csv&gid=')\n",
    "\n",
    "df_googlesheet = pd.read_csv(csv_export_url)\n",
    "df_googlesheet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###    -print the first 3 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_googlesheet.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###    -print the number of rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_googlesheet.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###    -print the first five column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_googlesheet.columns[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternate print to list for ease of access\n",
    "df_googlesheet.columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###     -print the data type of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_googlesheet.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###     -print the summary statistics for each of the numeric variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_googlesheet._get_numeric_data().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   -print the unique values for each of your categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_googlesheet.select_dtypes(object)\n",
    "\n",
    "df_ = df_googlesheet.select_dtypes(exclude=['int', 'float'])\n",
    "for col in df_.columns:\n",
    "    print(df_[col].unique()) # to print categories name only\n",
    "    print(df_[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alternate from faith\n",
    "for col in df_googlesheet:\n",
    "    if df_googlesheet[col].dtypes == 'object':\n",
    "        print(f'{col} has {df_googlesheet[col].nunique()} unique values.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a new python module, acquire.py to hold the following data aquisition functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Make a function named get_titanic_data that returns the titanic data from the codeup data science database as a pandas data frame. Obtain your data from the Codeup Data Science Database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import env\n",
    "\n",
    "def get_connection(db, user=env.user, host=env.host, password=env.password):\n",
    "    return f'mysql+pymysql://{user}:{password}@{host}/{db}'\n",
    "\n",
    "df = pd.read_sql('SELECT * FROM passengers', get_connection('titanic_db'))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Make a function named get_iris_data that returns the data from the iris_db on the codeup data science database as a pandas data frame. The returned data frame should include the actual name of the species in addition to the species_ids. Obtain your data from the Codeup Data Science Database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iris_data():\n",
    "    return pd.read_sql('SELECT * FROM measurements AS m JOIN species AS s on m.species_id = s.species_id', get_connection('iris_db'))\n",
    "\n",
    "df = get_iris_data()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Once you've got your get_titanic_data and get_iris_data functions written, now it's time to add caching to them. To do this, edit the beginning of the function to check for a local filename like titanic.csv or iris.csv. If they exist, use the .csv file. If the file doesn't exist, then produce the SQL and pandas necessary to create a dataframe, then write the dataframe to a .csv file with the appropriate name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Titanic Dataset\n",
    "\n",
    "# def get_titanic_data():\n",
    "#     filename = \"titanic.csv\"\n",
    "\n",
    "#     if os.path.isfile(filename):\n",
    "#         return pd.read_csv(filename)\n",
    "#     else:\n",
    "#         # read the SQL query into a dataframe\n",
    "#         df = pd.read_sql('SELECT * FROM passengers', get_connection('titanic_db'))\n",
    "\n",
    "#         # Write that dataframe to disk for later. Called \"caching\" the data for later.\n",
    "#         df.to_csv(filename, index = False)\n",
    "\n",
    "#         # Return the dataframe to the calling code\n",
    "#     return df\n",
    "\n",
    "#import acquire\n",
    "#acquire.get_titanic_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iris Dataset\n",
    "\n",
    "# def get_iris_data():\n",
    "#     filename = \"iris.csv\"\n",
    "\n",
    "#     if os.path.isfile(filename):\n",
    "#        return pd.read_csv(filename)\n",
    "#     else:\n",
    "#         #read the SQL query into a dataframe\n",
    "#         df = pd.read_sql('SELECT * FROM measurements AS m JOIN species AS s on m.species_id = s.species_id', get_connection('iris_db'))\n",
    "\n",
    "#         #Write that dataframe to disk for later. Called \"caching\" the data for later.\n",
    "#         df.to_csv(filename, index = False)\n",
    "\n",
    "#         #Return the dataframe to the calling code\n",
    "#     return df\n",
    "\n",
    "#import acquire\n",
    "#acquire.get_iris_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Prepare Enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from acquire import *\n",
    "from prepare import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- Iris Data\n",
    "\n",
    "    - Use the function defined in acquire.py to load the iris data.\n",
    "    - Drop the species_id and measurement_id columns.\n",
    "    - Rename the species_name column to just species.\n",
    "    - Encode the species name using a sklearn label encoder. Research the inverse_transform method of the label encoder. How might this be useful?\n",
    "    - Create a function named prep_iris that accepts the untransformed iris data, and returns the data with the transformations above applied.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species_id</th>\n",
       "      <th>species_name</th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>setosa</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>setosa</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>setosa</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>setosa</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>setosa</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   species_id species_name  sepal_length  sepal_width  petal_length  \\\n",
       "0           1       setosa           5.1          3.5           1.4   \n",
       "1           1       setosa           4.9          3.0           1.4   \n",
       "2           1       setosa           4.7          3.2           1.3   \n",
       "3           1       setosa           4.6          3.1           1.5   \n",
       "4           1       setosa           5.0          3.6           1.4   \n",
       "\n",
       "   petal_width  \n",
       "0          0.2  \n",
       "1          0.2  \n",
       "2          0.2  \n",
       "3          0.2  \n",
       "4          0.2  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \n",
    "'''\n",
    "This function reads in iris data from Codeup database if cached == False\n",
    "or if cached == True reads in iris df from a csv file, returns df\n",
    "'''\n",
    "# I ran this code first, to import data and save it as a dataframe\n",
    "#iris = get_iris_data(cached=True)\n",
    "\n",
    "iris = get_iris_data(cached=True)\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the species_id and measurement_id columns.\n",
    "\n",
    "cols_to_drop = ['species_id']\n",
    "iris = iris.drop(columns=cols_to_drop)\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the species_name column to just species.\n",
    "\n",
    "iris.rename(columns = {'species_name':'species'}, inplace = True)\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Validate/Test Split\n",
    "\n",
    "train_validate, test = train_test_split(iris, test_size=.2, \n",
    "                                        random_state=123, \n",
    "                                        stratify=iris.species)\n",
    "\n",
    "train_validate.shape, test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate = train_test_split(train_validate, test_size=.3, \n",
    "                                   random_state=123, \n",
    "                                   stratify=train_validate.species)\n",
    "train.shape, validate.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the species name using a sklearn label encoder. \n",
    "# Research the inverse_transform method of the label encoder. How might this be useful?\n",
    "\n",
    "def label_encode(train, test):\n",
    "    le = LabelEncoder()\n",
    "    train['species'] = le.fit_transform(train.species)\n",
    "    test['species'] = le.transform(test.species)\n",
    "    return le, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encode(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Research the inverse_transform method of the label encoder. How might this be useful?\n",
    "\n",
    "'''\n",
    "Used to transform the data back in to categories after modeling for analysis\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function named prep_iris that accepts the untransformed iris data, and returns the data with the transformations above applied.\n",
    "\n",
    "def prep_iris(df):\n",
    "    le = LabelEncoder()\n",
    "    df = df.drop(columns='species_id')\n",
    "    df = df.rename(columns={'species_name': 'species'})\n",
    "    train, test = train_test_split(df, train_size=.75, stratify=df.species, random_state=123)\n",
    "    train, test, le = label_encode(train, test)\n",
    "    return train, test, le\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LabelEncoder(),\n",
       "      species  sepal_length  sepal_width  petal_length  petal_width\n",
       " 30         0           4.8          3.1           1.6          0.2\n",
       " 36         0           5.5          3.5           1.3          0.2\n",
       " 29         0           4.7          3.2           1.6          0.2\n",
       " 55         1           5.7          2.8           4.5          1.3\n",
       " 118        2           7.7          2.6           6.9          2.3\n",
       " ..       ...           ...          ...           ...          ...\n",
       " 11         0           4.8          3.4           1.6          0.2\n",
       " 0          0           5.1          3.5           1.4          0.2\n",
       " 104        2           6.5          3.0           5.8          2.2\n",
       " 7          0           5.0          3.4           1.5          0.2\n",
       " 147        2           6.5          3.0           5.2          2.0\n",
       " \n",
       " [112 rows x 5 columns],\n",
       "      species  sepal_length  sepal_width  petal_length  petal_width\n",
       " 135        2           7.7          3.0           6.1          2.3\n",
       " 34         0           4.9          3.1           1.5          0.2\n",
       " 61         1           5.9          3.0           4.2          1.5\n",
       " 117        2           7.7          3.8           6.7          2.2\n",
       " 42         0           4.4          3.2           1.3          0.2\n",
       " 38         0           4.4          3.0           1.3          0.2\n",
       " 65         1           6.7          3.1           4.4          1.4\n",
       " 125        2           7.2          3.2           6.0          1.8\n",
       " 80         1           5.5          2.4           3.8          1.1\n",
       " 19         0           5.1          3.8           1.5          0.3\n",
       " 64         1           5.6          2.9           3.6          1.3\n",
       " 33         0           5.5          4.2           1.4          0.2\n",
       " 115        2           6.4          3.2           5.3          2.3\n",
       " 146        2           6.3          2.5           5.0          1.9\n",
       " 94         1           5.6          2.7           4.2          1.3\n",
       " 116        2           6.5          3.0           5.5          1.8\n",
       " 28         0           5.2          3.4           1.4          0.2\n",
       " 32         0           5.2          4.1           1.5          0.1\n",
       " 9          0           4.9          3.1           1.5          0.1\n",
       " 17         0           5.1          3.5           1.4          0.3\n",
       " 40         0           5.0          3.5           1.3          0.3\n",
       " 22         0           4.6          3.6           1.0          0.2\n",
       " 93         1           5.0          2.3           3.3          1.0\n",
       " 144        2           6.7          3.3           5.7          2.5\n",
       " 2          0           4.7          3.2           1.3          0.2\n",
       " 77         1           6.7          3.0           5.0          1.7\n",
       " 122        2           7.7          2.8           6.7          2.0\n",
       " 138        2           6.0          3.0           4.8          1.8\n",
       " 110        2           6.5          3.2           5.1          2.0\n",
       " 56         1           6.3          3.3           4.7          1.6\n",
       " 66         1           5.6          3.0           4.5          1.5\n",
       " 101        2           5.8          2.7           5.1          1.9\n",
       " 68         1           6.2          2.2           4.5          1.5\n",
       " 76         1           6.8          2.8           4.8          1.4\n",
       " 105        2           7.6          3.0           6.6          2.1\n",
       " 86         1           6.7          3.1           4.7          1.5\n",
       " 127        2           6.1          3.0           4.9          1.8\n",
       " 92         1           5.8          2.6           4.0          1.2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####     Use the function you defined in acquire.py to load the titanic data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function reads in titanic data from Codeup database if cached == False\n",
    "or if cached == True reads in iris df from a csv file, returns df\n",
    "'''\n",
    "# I ran this code first, to import data and save it as a dataframe\n",
    "#iris = get_iris_data(cached=True)\n",
    "\n",
    "titanic = get_titanic_data(cached=True)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####     Handle the missing values in the embark_town and embarked columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropped these colums because they are repeats, or do not provide extra insight in to who survived\n",
    "cols_to_drop = ['embarked', 'class', 'passenger_id']\n",
    "titanic = titanic.drop(columns=cols_to_drop)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop missing observations of embark town\n",
    "titanic = titanic[~titanic.embark_town.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####     Remove the deck column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['deck']\n",
    "titanic = titanic.drop(columns=cols_to_drop)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####    Use a label encoder to transform the embarked column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we have to train/validate/test split data\n",
    "\n",
    "train_validate, test = train_test_split(titanic, test_size=.2, \n",
    "                                        random_state=123, \n",
    "                                        stratify=titanic.survived)\n",
    "\n",
    "train, validate = train_test_split(train_validate, test_size=.3, \n",
    "                                   random_state=123, \n",
    "                                   stratify=train_validate.survived)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode(train, test):\n",
    "    le = LabelEncoder()\n",
    "    train['embark_town'] = le.fit_transform(train.embark_town)\n",
    "    test['embark_town'] = le.transform(test.embark_town)\n",
    "    return le, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encode(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####     Scale the age and fare columns using a min max scaler. Why might this be beneficial? When might you not want to do this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_columns(train, test):\n",
    "    scaler = MinMaxScaler()\n",
    "    train[['age','fare']] = scaler.fit_transform(train[['age','fare']])\n",
    "    test[['age','fare']] = scaler.transform(test[['age','fare']])\n",
    "    return scaler, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_columns(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####     Fill the missing values in age. The way you fill these values is up to you. Consider the tradeoffs of different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_age(train, test):\n",
    "    avg_age = train.age.mean()\n",
    "    train.age = train.age.fillna(avg_age)\n",
    "    test.age = test.age.fillna(avg_age)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_age(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####     Create a function named prep_titanic that accepts the untransformed titanic data, and returns the data with the   transformations above applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = get_titanic_data(cached=True)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(df):\n",
    "    df.drop(columns=['deck'], inplace=True)\n",
    "    return df\n",
    "\n",
    "def impute_embark_town(train, test):\n",
    "    train['embark_town'] = train['embark_town'].fillna('Southampton')\n",
    "    test['embark_town'] = test['embark_town'].fillna('Southampton')\n",
    "    return train, test\n",
    "\n",
    "def impute_embarked(train, test):\n",
    "    train['embarked'] = train['embarked'].fillna('S')\n",
    "    test['embarked'] = test['embarked'].fillna('S')\n",
    "    return train, test\n",
    "\n",
    "def impute_age(train, test):\n",
    "    avg_age = train.age.mean()\n",
    "    train.age = train.age.fillna(avg_age)\n",
    "    test.age = test.age.fillna(avg_age)\n",
    "    return train, test\n",
    "\n",
    "def scale_columns(train, test):\n",
    "    scaler = MinMaxScaler()\n",
    "    train[['age','fare']] = scaler.fit_transform(train[['age','fare']])\n",
    "    test[['age','fare']] = scaler.transform(test[['age','fare']])\n",
    "    return scaler, train, test\n",
    "\n",
    "def ohe_columns(train, test):\n",
    "    # create encoder\n",
    "    ohe = OneHotEncoder(sparse=False, categories='auto')\n",
    "    \n",
    "    # fit scaler on train and transform train and test to dense matrices\n",
    "    train_matrix = ohe.fit_transform(train[['embarked']])\n",
    "    test_matrix = ohe.transform(test[['embarked']])\n",
    "    \n",
    "    # transform matrices to DataFrames\n",
    "    train_ohe = pd.DataFrame(train_matrix, columns=ohe.categories_[0], index=train.index)\n",
    "    test_ohe = pd.DataFrame(test_matrix, columns=ohe.categories_[0], index=test.index)\n",
    "    \n",
    "    # join encoded matrix with original train or test matrices\n",
    "    train = train.join(train_ohe)\n",
    "    test = test.join(test_ohe)\n",
    "    \n",
    "    return ohe, train, test\n",
    "\n",
    "def prep_titanic(df):\n",
    "\n",
    "    # drop the deck column bc most values Null\n",
    "    drop_columns(df)\n",
    "    \n",
    "    train, test = train_test_split(df, train_size=.75, stratify=df.survived, random_state=123)\n",
    "    \n",
    "    # impute 2 NaNs in embark_town with most frequent value\n",
    "    train, test = impute_embark_town(train, test)\n",
    "    \n",
    "    # impute 2 NaNs in embarked with most frequent value\n",
    "    train, test = impute_embarked(train, test)\n",
    "    \n",
    "    # impute NaNs in age in train and test with the mean age in train\n",
    "    train, test = impute_age(train, test)\n",
    "    \n",
    "    # use a minmax scaler on age and fare bc of differing measurement units\n",
    "    scaler, train, test = scale_columns(train, test)\n",
    "    \n",
    "    # ohe embarked creating three new columns for C, Q, S representing embark towns\n",
    "    ohe, train, test = ohe_columns(train, test)\n",
    "    \n",
    "    return scaler, ohe, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'titanic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b269cc52779f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprep_titanic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitanic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'titanic' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
