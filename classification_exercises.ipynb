{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Exercises \n",
    "#### Corey Solitaire\n",
    "#### 9/8/2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from math import sqrt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 In a jupyter notebook, classification_exercises.ipynb, use a python module (pydata or seaborn datasets) containing datasets as a source from the iris data. Create a pandas dataframe, df_iris, from this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = sns.load_dataset('iris')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   - print the first 3 rows\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###     - print the number of rows and columns (shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###    - print the column names\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###    - print the data type of each column\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#more info is necessary\n",
    "iris.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###     - print the summary statistics for each of the numeric variables. Would you recommend rescaling the data based on these statistics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.sepal_length.var(),\n",
    "iris.sepal_width.var(),\n",
    "iris.petal_length.var(),\n",
    "iris.petal_width.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Petal length is much more variable then other measurments, I might reccomend scaling depending on the factors being examined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Read the Table1_CustDetails table from the Excel_Exercises.xlsx file into a dataframe named df_excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer = pd.read_excel('Spreadsheets_Exercises.xlsx')\n",
    "customer.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###      -assign the first 100 rows to a new dataframe, df_excel_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_excel_sample = customer.head(100)\n",
    "df_excel_sample.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###      -print the number of rows of your original dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(customer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alternate option\n",
    "customer.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###      -print the first 5 column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Select First 5 columns:\")\n",
    "print(customer[['customer_id', 'gender', 'is_senior_citizen', 'partner', 'dependents']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternate method\n",
    "\n",
    "customer.columns[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###      -print the column names that have a data type of object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredColumns = customer.dtypes[customer.dtypes == np.object]\n",
    "listOfColumnNames = list(filteredColumns.index)\n",
    "print(listOfColumnNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###     -compute the range for each of the numeric variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transcribes table\n",
    "stats =customer._get_numeric_data().describe().T\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats['range'] = stats['max'] - stats['min']\n",
    "stats[['mean', '50%', 'std', 'range']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Read the data from this google sheet into a dataframe, df_google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_url = 'https://docs.google.com/spreadsheets/d/1Uhtml8KY19LILuZsrDtlsHHDC9wuDGUSe8LTEwvdI5g/edit#gid=341089357'    \n",
    "\n",
    "csv_export_url = sheet_url.replace('/edit#gid=', '/export?format=csv&gid=')\n",
    "\n",
    "df_googlesheet = pd.read_csv(csv_export_url)\n",
    "df_googlesheet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###    -print the first 3 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_googlesheet.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###    -print the number of rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_googlesheet.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###    -print the first five column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_googlesheet.columns[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternate print to list for ease of access\n",
    "df_googlesheet.columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###     -print the data type of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_googlesheet.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###     -print the summary statistics for each of the numeric variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_googlesheet._get_numeric_data().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   -print the unique values for each of your categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_googlesheet.select_dtypes(object)\n",
    "\n",
    "df_ = df_googlesheet.select_dtypes(exclude=['int', 'float'])\n",
    "for col in df_.columns:\n",
    "    print(df_[col].unique()) # to print categories name only\n",
    "    print(df_[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alternate from faith\n",
    "for col in df_googlesheet:\n",
    "    if df_googlesheet[col].dtypes == 'object':\n",
    "        print(f'{col} has {df_googlesheet[col].nunique()} unique values.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a new python module, acquire.py to hold the following data aquisition functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Make a function named get_titanic_data that returns the titanic data from the codeup data science database as a pandas data frame. Obtain your data from the Codeup Data Science Database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import env\n",
    "\n",
    "def get_connection(db, user=env.user, host=env.host, password=env.password):\n",
    "    return f'mysql+pymysql://{user}:{password}@{host}/{db}'\n",
    "\n",
    "df = pd.read_sql('SELECT * FROM passengers', get_connection('titanic_db'))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Make a function named get_iris_data that returns the data from the iris_db on the codeup data science database as a pandas data frame. The returned data frame should include the actual name of the species in addition to the species_ids. Obtain your data from the Codeup Data Science Database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iris_data():\n",
    "    return pd.read_sql('SELECT * FROM measurements AS m JOIN species AS s on m.species_id = s.species_id', get_connection('iris_db'))\n",
    "\n",
    "df = get_iris_data()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Once you've got your get_titanic_data and get_iris_data functions written, now it's time to add caching to them. To do this, edit the beginning of the function to check for a local filename like titanic.csv or iris.csv. If they exist, use the .csv file. If the file doesn't exist, then produce the SQL and pandas necessary to create a dataframe, then write the dataframe to a .csv file with the appropriate name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Titanic Dataset\n",
    "\n",
    "# def get_titanic_data():\n",
    "#     filename = \"titanic.csv\"\n",
    "\n",
    "#     if os.path.isfile(filename):\n",
    "#         return pd.read_csv(filename)\n",
    "#     else:\n",
    "#         # read the SQL query into a dataframe\n",
    "#         df = pd.read_sql('SELECT * FROM passengers', get_connection('titanic_db'))\n",
    "\n",
    "#         # Write that dataframe to disk for later. Called \"caching\" the data for later.\n",
    "#         df.to_csv(filename, index = False)\n",
    "\n",
    "#         # Return the dataframe to the calling code\n",
    "#     return df\n",
    "\n",
    "#import acquire\n",
    "#acquire.get_titanic_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iris Dataset\n",
    "\n",
    "# def get_iris_data():\n",
    "#     filename = \"iris.csv\"\n",
    "\n",
    "#     if os.path.isfile(filename):\n",
    "#        return pd.read_csv(filename)\n",
    "#     else:\n",
    "#         #read the SQL query into a dataframe\n",
    "#         df = pd.read_sql('SELECT * FROM measurements AS m JOIN species AS s on m.species_id = s.species_id', get_connection('iris_db'))\n",
    "\n",
    "#         #Write that dataframe to disk for later. Called \"caching\" the data for later.\n",
    "#         df.to_csv(filename, index = False)\n",
    "\n",
    "#         #Return the dataframe to the calling code\n",
    "#     return df\n",
    "\n",
    "#import acquire\n",
    "#acquire.get_iris_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Prepare Enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from acquire import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- Iris Data\n",
    "\n",
    "    - Use the function defined in acquire.py to load the iris data.\n",
    "    - Drop the species_id and measurement_id columns.\n",
    "    - Rename the species_name column to just species.\n",
    "    - Encode the species name using a sklearn label encoder. Research the inverse_transform method of the label encoder. How might this be useful?\n",
    "    - Create a function named prep_iris that accepts the untransformed iris data, and returns the data with the transformations above applied.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "'''\n",
    "This function reads in iris data from Codeup database if cached == False\n",
    "or if cached == True reads in iris df from a csv file, returns df\n",
    "'''\n",
    "# I ran this code first, to import data and save it as a dataframe\n",
    "#iris = get_iris_data(cached=True)\n",
    "\n",
    "iris = get_iris_data(cached=True)\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the species_id and measurement_id columns.\n",
    "\n",
    "cols_to_drop = ['species_id']\n",
    "iris = iris.drop(columns=cols_to_drop)\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the species_name column to just species.\n",
    "\n",
    "iris.rename(columns = {'species_name':'species'}, inplace = True)\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Validate/Test Split\n",
    "\n",
    "train_validate, test = train_test_split(iris, test_size=.2, \n",
    "                                        random_state=123, \n",
    "                                        stratify=iris.species)\n",
    "\n",
    "train_validate.shape, test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate = train_test_split(train_validate, test_size=.3, \n",
    "                                   random_state=123, \n",
    "                                   stratify=train_validate.species)\n",
    "train.shape, validate.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the species name using a sklearn label encoder. \n",
    "# Research the inverse_transform method of the label encoder. How might this be useful?\n",
    "\n",
    "def label_encode(train, test):\n",
    "    le = LabelEncoder()\n",
    "    train['species'] = le.fit_transform(train.species)\n",
    "    test['species'] = le.transform(test.species)\n",
    "    return le, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encode(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Research the inverse_transform method of the label encoder. How might this be useful?\n",
    "\n",
    "'''\n",
    "Used to transform the data back in to categories after modeling for analysis\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function named prep_iris that accepts the untransformed iris data, and returns the data with the transformations above applied.\n",
    "\n",
    "def prep_iris(df):\n",
    "    le = LabelEncoder()\n",
    "    df = df.drop(columns='species_id')\n",
    "    df = df.rename(columns={'species_name': 'species'})\n",
    "    train, test = train_test_split(df, train_size=.75, stratify=df.species, random_state=123)\n",
    "    train, test, le = label_encode(train, test)\n",
    "    return train, test, le\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####     Use the function you defined in acquire.py to load the titanic data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function reads in titanic data from Codeup database if cached == False\n",
    "or if cached == True reads in iris df from a csv file, returns df\n",
    "'''\n",
    "# I ran this code first, to import data and save it as a dataframe\n",
    "#iris = get_iris_data(cached=True)\n",
    "\n",
    "titanic = get_titanic_data(cached=True)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####     Handle the missing values in the embark_town and embarked columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropped these colums because they are repeats, or do not provide extra insight in to who survived\n",
    "cols_to_drop = ['embarked', 'class', 'passenger_id']\n",
    "titanic = titanic.drop(columns=cols_to_drop)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop missing observations of embark town\n",
    "titanic = titanic[~titanic.embark_town.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####     Remove the deck column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['deck']\n",
    "titanic = titanic.drop(columns=cols_to_drop)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####    Use a label encoder to transform the embarked column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we have to train/validate/test split data\n",
    "\n",
    "train_validate, test = train_test_split(titanic, test_size=.2, \n",
    "                                        random_state=123, \n",
    "                                        stratify=titanic.survived)\n",
    "\n",
    "train, validate = train_test_split(train_validate, test_size=.3, \n",
    "                                   random_state=123, \n",
    "                                   stratify=train_validate.survived)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode(train, test):\n",
    "    le = LabelEncoder()\n",
    "    train['embark_town'] = le.fit_transform(train.embark_town)\n",
    "    test['embark_town'] = le.transform(test.embark_town)\n",
    "    return le, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encode(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####     Scale the age and fare columns using a min max scaler. Why might this be beneficial? When might you not want to do this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_columns(train, test):\n",
    "    scaler = MinMaxScaler()\n",
    "    train[['age','fare']] = scaler.fit_transform(train[['age','fare']])\n",
    "    test[['age','fare']] = scaler.transform(test[['age','fare']])\n",
    "    return scaler, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_columns(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####     Fill the missing values in age. The way you fill these values is up to you. Consider the tradeoffs of different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_age(train, test):\n",
    "    avg_age = train.age.mean()\n",
    "    train.age = train.age.fillna(avg_age)\n",
    "    test.age = test.age.fillna(avg_age)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_age(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####     Create a function named prep_titanic that accepts the untransformed titanic data, and returns the data with the   transformations above applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = get_titanic_data(cached=True)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prep_titanic_2\n",
    "\n",
    "def drop_edit_columns(df):\n",
    "    titanic = titanic[~titanic.embark_town.isnull()]\n",
    "    cols_to_drop = ['embarked', 'class', 'deck', 'passenger_id']\n",
    "    titanic = titanic.drop(columns=cols_to_drop)\n",
    "    titanic.head()\n",
    "    return df\n",
    "\n",
    "def test_split(df):\n",
    "    train_validate, test = train_test_split(titanic, test_size=.2, \n",
    "                                        random_state=123, \n",
    "                                        stratify=titanic.survived)\n",
    "    train, validate = train_test_split(train_validate, test_size=.3, \n",
    "                                   random_state=123, \n",
    "                                   stratify=train_validate.survived)\n",
    "    return df\n",
    "\n",
    "def label_encode(train, test):\n",
    "    le = LabelEncoder()\n",
    "    train['embark_town'] = le.fit_transform(train.embark_town)\n",
    "    test['embark_town'] = le.transform(test.embark_town)\n",
    "    return le, train, test\n",
    "\n",
    "def scale_columns(train, test):\n",
    "    scaler = MinMaxScaler()\n",
    "    train[['age','fare']] = scaler.fit_transform(train[['age','fare']])\n",
    "    test[['age','fare']] = scaler.transform(test[['age','fare']])\n",
    "    return scaler, train, test\n",
    "\n",
    "def impute_age(train, test):\n",
    "    avg_age = train.age.mean()\n",
    "    train.age = train.age.fillna(avg_age)\n",
    "    test.age = test.age.fillna(avg_age)\n",
    "    return train, test\n",
    "\n",
    "def prep_titanic_2(df):\n",
    "\n",
    "    # drop the deck column bc most values Null\n",
    "    drop_edit_columns\n",
    "    \n",
    "    train, test = train_test_split(df, train_size=.75, stratify=df.survived, random_state=123)\n",
    "    \n",
    "    # impute embarked town to 0,1,2\n",
    "    train, test = label_encode_town(train, test)\n",
    "    \n",
    "    # scale age and fare columns\n",
    "    train, test = scale_columns(train, test)\n",
    "    \n",
    "    # impute NaNs in age in train and test with the mean age in train\n",
    "    train, test = impute_age(train, test)\n",
    "    \n",
    "    return scaler, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(df):\n",
    "    df.drop(columns=['deck'], inplace=True)\n",
    "    return df\n",
    "\n",
    "def impute_embark_town(train, test):\n",
    "    train['embark_town'] = train['embark_town'].fillna('Southampton')\n",
    "    test['embark_town'] = test['embark_town'].fillna('Southampton')\n",
    "    return train, test\n",
    "\n",
    "def impute_embarked(train, test):\n",
    "    train['embarked'] = train['embarked'].fillna('S')\n",
    "    test['embarked'] = test['embarked'].fillna('S')\n",
    "    return train, test\n",
    "\n",
    "def impute_age(train, test):\n",
    "    avg_age = train.age.mean()\n",
    "    train.age = train.age.fillna(avg_age)\n",
    "    test.age = test.age.fillna(avg_age)\n",
    "    return train, test\n",
    "\n",
    "def scale_columns(train, test):\n",
    "    scaler = MinMaxScaler()\n",
    "    train[['age','fare']] = scaler.fit_transform(train[['age','fare']])\n",
    "    test[['age','fare']] = scaler.transform(test[['age','fare']])\n",
    "    return scaler, train, test\n",
    "\n",
    "def ohe_columns(train, test):\n",
    "    # create encoder\n",
    "    ohe = OneHotEncoder(sparse=False, categories='auto')\n",
    "    \n",
    "    # fit scaler on train and transform train and test to dense matrices\n",
    "    train_matrix = ohe.fit_transform(train[['embarked']])\n",
    "    test_matrix = ohe.transform(test[['embarked']])\n",
    "    \n",
    "    # transform matrices to DataFrames\n",
    "    train_ohe = pd.DataFrame(train_matrix, columns=ohe.categories_[0], index=train.index)\n",
    "    test_ohe = pd.DataFrame(test_matrix, columns=ohe.categories_[0], index=test.index)\n",
    "    \n",
    "    # join encoded matrix with original train or test matrices\n",
    "    train = train.join(train_ohe)\n",
    "    test = test.join(test_ohe)\n",
    "    \n",
    "    return ohe, train, test\n",
    "\n",
    "def prep_titanic(df):\n",
    "\n",
    "    # drop the deck column bc most values Null\n",
    "    drop_columns(df)\n",
    "    \n",
    "    train, test = train_test_split(df, train_size=.75, stratify=df.survived, random_state=123)\n",
    "    \n",
    "    # impute 2 NaNs in embark_town with most frequent value\n",
    "    train, test = impute_embark_town(train, test)\n",
    "    \n",
    "    # impute 2 NaNs in embarked with most frequent value\n",
    "    train, test = impute_embarked(train, test)\n",
    "    \n",
    "    # impute NaNs in age in train and test with the mean age in train\n",
    "    train, test = impute_age(train, test)\n",
    "    \n",
    "    # use a minmax scaler on age and fare bc of differing measurement units\n",
    "    scaler, train, test = scale_columns(train, test)\n",
    "    \n",
    "    # ohe embarked creating three new columns for C, Q, S representing embark towns\n",
    "    ohe, train, test = ohe_columns(train, test)\n",
    "    \n",
    "    return scaler, ohe, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
