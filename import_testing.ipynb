{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import acquire\n",
    "import prepare\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from acquire import get_titanic_data\n",
    "from prepare import prep_titanic_data_beta\n",
    "\n",
    "# New datafram Test\n",
    "X_train, X_validate, X_test, y_train, y_validate, y_test = prep_titanic_data_beta(get_titanic_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  (498, 5) , validate:  (214, 5) , test:  (179, 5)\n",
      "train:  (498, 1) , validate:  (214, 1) , test:  (179, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"train: \", X_train.shape, \", validate: \", X_validate.shape, \", test: \", X_test.shape)\n",
    "print(\"train: \", y_train.shape, \", validate: \", y_validate.shape, \", test: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression classifier on training set: 0.79\n"
     ]
    }
   ],
   "source": [
    "logit = LogisticRegression(C=1, random_state = 123, solver='newton-cg')\n",
    "logit.fit(X_train, y_train)\n",
    "#make a prediction with traning data\n",
    "y_pred = logit.predict(X_train)\n",
    "#estimate prob of survival with training data\n",
    "y_pred_proba = logit.predict_proba(X_train)\n",
    "# Compute Accuracy\n",
    "print('Accuracy of Logistic Regression classifier on training set: {:.2f}'.format(logit.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaned Data for Exploration\n",
    "df = acquire.get_telco_data(cached = True)\n",
    "train, validate, test = prepare.prep_telco_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats = train.describe().T\n",
    "train_stats['range'] = train_stats['max'] - train_stats['min']\n",
    "train_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix (all train)\n",
    "corr = train.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, annot = True, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(train, hue='churn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(train, hue='contract_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_rate = train.churn.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.churn.value_counts().plot.bar()\n",
    "plt.xlabel('Churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby('gender').churn.mean().plot.bar(alpha=.8)\n",
    "plt.ylabel('Churn Rate')\n",
    "plt.hlines(churn_rate, *plt.xlim(), ls='--', alpha=.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby('multiple_lines').churn.mean().plot.bar(alpha=.8)\n",
    "plt.ylabel('Churn Rate')\n",
    "plt.hlines(churn_rate, *plt.xlim(), ls='--', alpha=.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby('internet_service').churn.mean().plot.bar(alpha=.8)\n",
    "plt.ylabel('Churn Rate')\n",
    "plt.hlines(churn_rate, *plt.xlim(), ls='--', alpha=.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby('phone_service').churn.mean().plot.bar(alpha=.8)\n",
    "plt.ylabel('Churn Rate')\n",
    "plt.hlines(churn_rate, *plt.xlim(), ls='--', alpha=.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby('internet_service_type').churn.mean().plot.bar(alpha=.8)\n",
    "plt.ylabel('Churn Rate')\n",
    "plt.hlines(churn_rate, *plt.xlim(), ls='--', alpha=.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby('online_security').churn.mean().plot.bar(alpha=.8)\n",
    "plt.ylabel('Churn Rate')\n",
    "plt.hlines(churn_rate, *plt.xlim(), ls='--', alpha=.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby('online_backup').churn.mean().plot.bar(alpha=.8)\n",
    "plt.ylabel('Churn Rate')\n",
    "plt.hlines(churn_rate, *plt.xlim(), ls='--', alpha=.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby('contract_type').churn.mean().plot.bar(alpha=.8)\n",
    "plt.ylabel('Churn Rate')\n",
    "plt.hlines(churn_rate, *plt.xlim(), ls='--', alpha=.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby('payment_type').churn.mean().plot.bar(alpha=.8)\n",
    "plt.ylabel('Churn Rate')\n",
    "plt.hlines(churn_rate, *plt.xlim(), ls='--', alpha=.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(train.churn, train.payment_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(train.pivot_table('churn', 'contract_type', 'internet_service_type'), cmap='Blues', annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(train.pivot_table('churn', 'contract_type', 'payment_type'), cmap='Blues', annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(train.pivot_table('churn', 'contract_type', 'streaming_movies'), cmap='Blues', annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(train.pivot_table('churn', 'contract_type', 'streaming_tv'), cmap='Blues', annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(train.pivot_table('churn', 'contract_type', 'tech_support'), cmap='Blues', annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(train.pivot_table('churn', 'contract_type', 'device_protection'), cmap='Blues', annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(train.pivot_table('churn', 'contract_type', 'online_backup'), cmap='Blues', annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(train.pivot_table('churn', 'contract_type', 'online_security'), cmap='Blues', annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(train.pivot_table('churn', 'contract_type', 'multiple_lines'), cmap='Blues', annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(train.pivot_table('churn', 'contract_type', 'paperless_billing'), cmap='Blues', annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    " - In month to month contracts correlation between churn and multiple phone lines(0.48)\n",
    " - In month to month contracts correlation between churn and no online security (0.5)\n",
    " - In month to month contracts correlation between churn and no tech support (0.5)\n",
    " - In month to month contracts correlation between churn and electronic check payment type (0.53)\n",
    " - In month to month contracts correlation between churn and fiber optic internet service (0.54)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(13, 7))\n",
    "sns.swarmplot(data=train, y='monthly_charges', x='internet_service_type', hue='churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix (all train)\n",
    "corr = train.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, annot = True, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[['churn', 'gender', 'senior_citizen', 'partner', 'dependents', 'month_to_month_contract', 'internet_service_type']]\n",
    "\n",
    "# Compute a correlation matrix and convert to long-form\n",
    "corr_mat = df.corr().stack().reset_index(name=\"correlation\")\n",
    "\n",
    "# Draw each cell as a scatter point with varying size and color\n",
    "g = sns.relplot(\n",
    "    data=corr_mat,\n",
    "    x=\"level_0\", y=\"level_1\", hue=\"correlation\", size=\"correlation\",\n",
    "    palette=\"vlag\", hue_norm=(-1, 1), edgecolor=\".7\",\n",
    "    height=10, sizes=(50, 250), size_norm=(-.2, .8),\n",
    ")\n",
    "\n",
    "# # Tweak the figure to finalize\n",
    "# g.set(xlabel=\"\", ylabel=\"\", aspect=\"equal\")\n",
    "# g.despine(left=True, bottom=True)\n",
    "# g.ax.margins(.02)\n",
    "# for label in g.ax.get_xticklabels():\n",
    "#     label.set_rotation(90)\n",
    "# for artist in g.legend.legendHandles:\n",
    "#     artist.set_edgecolor(\".7\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "corr = train.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, annot = True, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2nd Heatmap, not sure which to use...\n",
    "f, ax = plt.subplots(figsize=(9, 6))\n",
    "sns.heatmap(corr, annot=True, linewidths=.5, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.churn.var(), train.month_to_month_contract.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Null Hypothesis:\n",
    "\n",
    "$H_0$: Churn and month to month contracts are independent (not dependent)\n",
    "\n",
    "$H_a$: churn and month to month contracts are dependent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats testing\n",
    "\n",
    "observed = pd.crosstab(train.churn, train.month_to_month_contract)\n",
    "observed\n",
    "\n",
    "# Set our alpha\n",
    "# alpha nice and low\n",
    "alpha = .05\n",
    "\n",
    "# .chi2_contingency returns 4 different values\n",
    "chi2, p, degf, expected = stats.chi2_contingency(observed)\n",
    "\n",
    "null_hypothesis = \"Churn and month to month contracts are independent\"\n",
    "\n",
    "if p < alpha:\n",
    "    print(\"We reject the null hypothesis\")\n",
    "    print(\"We reject the hypothesis that\", null_hypothesis)\n",
    "else:\n",
    "    print(\"We fail to reject the null hypothesis\")\n",
    "\n",
    "print(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null Hypothesis:\n",
    "\n",
    "$H_0$: Churn and being a senior citizen are independent (not dependent)\n",
    "\n",
    "$H_a$: Churn and being a senior citizen  are dependent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats testing\n",
    "\n",
    "observed = pd.crosstab(train.churn, train.senior_citizen)\n",
    "observed\n",
    "\n",
    "# Set our alpha\n",
    "# alpha nice and low\n",
    "alpha = .05\n",
    "\n",
    "# .chi2_contingency returns 4 different values\n",
    "chi2, p, degf, expected = stats.chi2_contingency(observed)\n",
    "\n",
    "null_hypothesis = \"Churn and being a senior citizen are independent\"\n",
    "\n",
    "if p < alpha:\n",
    "    print(\"We reject the null hypothesis\")\n",
    "    print(\"We reject the hypothesis that\", null_hypothesis)\n",
    "else:\n",
    "    print(\"We fail to reject the null hypothesis\")\n",
    "\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaned Data for Exploration\n",
    "df = acquire.get_telco_data(cached = True)\n",
    "train, validate, test = prepare.prep_telco_data(df)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd set to data to examine\n",
    "train = train[['churn','paperless_billing','streaming_movies', 'streaming_tv', 'tech_support', 'device_protection', 'online_backup', 'online_security', 'multiple_lines', 'monthly_charges']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "corr = train.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, annot = True, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null Hypothesis:\n",
    "\n",
    "𝐻0\n",
    "\n",
    ": Churn and paperless billing are independent (not dependent)\n",
    "\n",
    "𝐻𝑎\n",
    ": Churn and paperless billing are dependent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats testing\n",
    "\n",
    "observed = pd.crosstab(train.churn, train.paperless_billing)\n",
    "observed\n",
    "\n",
    "# Set our alpha\n",
    "# alpha nice and low\n",
    "alpha = .05\n",
    "\n",
    "# .chi2_contingency returns 4 different values\n",
    "chi2, p, degf, expected = stats.chi2_contingency(observed)\n",
    "\n",
    "null_hypothesis = \"Churn and paperless billing are independent\"\n",
    "\n",
    "if p < alpha:\n",
    "    print(\"We reject the null hypothesis\")\n",
    "    print(\"We reject the hypothesis that\", null_hypothesis)\n",
    "else:\n",
    "    print(\"We fail to reject the null hypothesis\")\n",
    "\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t, p = stats.ttest_ind(train.churn, train.monthly_charges, equal_var = True)\n",
    "print(f'''\n",
    "t = {t:.4f}\n",
    "p = {p:.8f}\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null Hypothesis:\n",
    "\n",
    "𝐻0\n",
    "\n",
    ": Churn and streaming tv are independent (not dependent)\n",
    "\n",
    "𝐻𝑎 : Churn and streaming tv  are dependent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats testing\n",
    "\n",
    "observed = pd.crosstab(train.churn, train.streaming_tv)\n",
    "observed\n",
    "\n",
    "# Set our alpha\n",
    "# alpha nice and low\n",
    "alpha = .05\n",
    "\n",
    "# .chi2_contingency returns 4 different values\n",
    "chi2, p, degf, expected = stats.chi2_contingency(observed)\n",
    "\n",
    "null_hypothesis = \"Churn and streaming tv are independent\"\n",
    "\n",
    "if p < alpha:\n",
    "    print(\"We reject the null hypothesis\")\n",
    "    print(\"We reject the hypothesis that\", null_hypothesis)\n",
    "else:\n",
    "    print(\"We fail to reject the null hypothesis\")\n",
    "\n",
    "print(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#titanic = acquire.get_titanic_data(cached=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train, validate, test = prepare.prep_titanic(titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validate.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Messing around with titanic data for exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from acquire import get_titanic_data\n",
    "from prepare import prep_titanic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prep_titanic_data(get_titanic_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.age.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# survival_rate = train.survived.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.survived.value_counts().plot.bar()\n",
    "# plt.xlabel('Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.groupby('sex').survived.mean().plot.bar(alpha=.8)\n",
    "# plt.ylabel('Survival Rate')\n",
    "# plt.hlines(survival_rate, *plt.xlim(), ls='--', alpha=.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby('parch').survived.mean().plot.bar(alpha=.8)\n",
    "plt.ylabel('Survival Rate')\n",
    "plt.hlines(survival_rate, *plt.xlim(), ls='--', alpha=.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Exploring 2 categorical variables, but now we're treating survived as a category, not a number.\n",
    "# pd.crosstab(train.survived, train.sex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.crosstab(train.survived, train.alone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.groupby('survived').age.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.groupby('survived').fare.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(13, 7))\n",
    "# sns.swarmplot(data=train, y='sex', x='age', hue='survived')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Messing around with the TELCO project dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to import data from SQL\n",
    "# Connect to employees database\n",
    "#defines function to create a sql url using personal credentials\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import acquire\n",
    "import prepare\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler\n",
    "from env import host, user, password\n",
    "\n",
    "def get_db_url(database, user=user, host=host, password=password): \n",
    "    url = f'mysql+pymysql://{user}:{password}@{host}/{database}'\n",
    "    return url\n",
    "\n",
    "url = get_db_url('telco_churn')\n",
    "\n",
    "query = '''\n",
    "select * \n",
    "from customers as c\n",
    "join contract_types as ct\n",
    "on ct.contract_type_id = c.contract_type_id\n",
    "join internet_service_types as i_s\n",
    "on i_s.internet_service_type_id = c.internet_service_type_id\n",
    "join payment_types as pt\n",
    "on pt.payment_type_id = c.payment_type_id;\n",
    "'''\n",
    "df = pd.read_sql(query, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Cleaned Data\n",
    "\n",
    "# Delete columns 'customer_id', contract_type_id, internet_service_type_id, payment_type_id    \n",
    "df.drop(columns = ['customer_id','contract_type_id','internet_service_type_id', 'payment_type_id'], inplace = True)\n",
    "# Replace partner, dependents, churn, phone_service, paperless billing, with boolean value\n",
    "df.partner.replace(['Yes', 'No'], [1,0], inplace = True)\n",
    "df.dependents.replace(['Yes', 'No'], [1,0], inplace = True)\n",
    "df.churn.replace(['Yes', 'No'], [1,0], inplace = True)\n",
    "df.phone_service.replace(['Yes', 'No'], [1,0], inplace = True)\n",
    "df.paperless_billing.replace(['Yes', 'No'], [1,0], inplace = True)\n",
    "# # Add dummy variables as new columns in dataframe and rename them, delete origional\n",
    "# gender = df.gender.str.get_dummies()\n",
    "# df = pd.concat([df, gender], axis=1)\n",
    "# df.rename(columns = {'Female': 'is_female', 'Male': 'is_male'}, inplace = True)\n",
    "# df.drop(columns = ['gender'], inplace = True)\n",
    "# # Add dummy variables as new columns in dataframe and rename them, delete origional\n",
    "# multiple = df.multiple_lines.str.get_dummies()\n",
    "# df = pd.concat([df, multiple], axis=1)\n",
    "# df.rename(columns = {'No': 'no_multiple_lines', 'Yes': 'yes_multiple_lines'}, inplace = True)\n",
    "# df.drop(columns = ['multiple_lines'], inplace = True)\n",
    "# # Add dummy variables as new columns in dataframe and rename them, delete origional\n",
    "# multiple = df.online_security.str.get_dummies()\n",
    "# df = pd.concat([df, multiple], axis=1)\n",
    "# df.rename(columns = {'No': 'no_online_security', 'Yes': 'yes_online_security'}, inplace = True)\n",
    "# df.drop(columns = ['online_security'], inplace = True)\n",
    "# # Add dummy variables as new columns in dataframe and rename them, delete origional\n",
    "# multiple = df.online_backup.str.get_dummies()\n",
    "# df = pd.concat([df, multiple], axis=1)\n",
    "# df.rename(columns = {'No': 'no_online_backup', 'Yes': 'yes_online_backup'}, inplace = True)\n",
    "# df.drop(columns = ['online_backup'], inplace = True)\n",
    "# # Add dummy variables as new columns in dataframe and rename them, delete origional\n",
    "# multiple = df.device_protection.str.get_dummies()\n",
    "# df = pd.concat([df, multiple], axis=1)\n",
    "# df.rename(columns = {'No': 'no_device_protection', 'Yes': 'yes_device_protection'}, inplace = True)\n",
    "# df.drop(columns = ['device_protection'], inplace = True)\n",
    "# # Add dummy variables as new columns in dataframe and rename them, delete origional\n",
    "# multiple = df.tech_support.str.get_dummies()\n",
    "# df = pd.concat([df, multiple], axis=1)\n",
    "# df.rename(columns = {'No': 'no_tech_support', 'Yes': 'yes_tech_support'}, inplace = True)\n",
    "# df.drop(columns = ['tech_support'], inplace = True)\n",
    "# # Add dummy variables as new columns in dataframe and rename them, delete origional\n",
    "# multiple = df.streaming_tv.str.get_dummies()\n",
    "# df = pd.concat([df, multiple], axis=1)\n",
    "# df.rename(columns = {'No': 'no_streaming_tv', 'Yes': 'yes_streaming_tv'}, inplace = True)\n",
    "# df.drop(columns = ['streaming_tv', 'No internet service'], inplace = True)\n",
    "# # Add dummy variables as new columns in dataframe and rename them, delete origional\n",
    "# multiple = df.streaming_movies.str.get_dummies()\n",
    "# df = pd.concat([df, multiple], axis=1)\n",
    "# df.rename(columns = {'No': 'no_streaming_movies', 'Yes': 'yes_streaming_movies'}, inplace = True)\n",
    "# df.drop(columns = ['streaming_movies'], inplace = True)\n",
    "# # Add dummy variables as new columns in dataframe and rename them, delete origional\n",
    "# multiple = df.contract_type.str.get_dummies()\n",
    "# df = pd.concat([df, multiple], axis=1)\n",
    "# df.rename(columns = {'Month-to-month': 'month_to_month_contract', 'One year': 'one_year_contract', 'Two year': 'two_year_contract'}, inplace = True)\n",
    "# df.drop(columns = ['contract_type'], inplace = True)\n",
    "# Add dummy variables as new columns in dataframe and rename them, delete origional\n",
    "#multiple = df.internet_service_type.str.get_dummies()\n",
    "#df = pd.concat([df, multiple], axis=1)\n",
    "#df.rename(columns = {'DSL': 'dsl', 'Fiber optic': 'fiber_optic'}, inplace = True)\n",
    "df['internet_service'] = df.internet_service_type != 'None'\n",
    "result = df['internet_service'].astype(int)\n",
    "df['internet_service'] = result\n",
    "#df.internet_service.replace(['Yes', 'No'], [1,0], inplace = True)\n",
    "#df.drop(columns = ['internet_service_type','None'], inplace = True)\n",
    "# # Add dummy variables as new columns in dataframe and rename them, delete origional\n",
    "# multiple = df.payment_type.str.get_dummies()\n",
    "# df = pd.concat([df, multiple], axis=1)\n",
    "# df.rename(columns = {'Bank transfer (automatic)': 'auto_bank_transfer', 'Credit card (automatic)': 'auto_credit_card', 'Electronic check': 'e_check', 'Mailed check': 'mail_check'}, inplace = True)\n",
    "# df.drop(columns = ['payment_type'], inplace = True)\n",
    "# # Change total_charges to float from object\n",
    "# df['total_charges'] = pd.to_numeric(df['total_charges'],errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.internet_service.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Things to Consider:\n",
    "1. How to handle colums that could be booleans (Yes/No/Null)\n",
    "   -Keep as object or turn to int ?\n",
    "   -Columns include '''\n",
    "   \n",
    "  \n",
    "\n",
    "''' contract_type\n",
    "    \n",
    "2. Total charges change to float, it is currently an object\n",
    "3. Delete origional dummy variable columns\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy multiple lines\n",
    "\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
